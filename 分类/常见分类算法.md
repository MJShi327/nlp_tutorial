###<center>常见的分类算法</center>
####1、朴素贝叶斯分类器
#####主要思想：
朴素贝叶斯法是基于贝叶斯定理与特征条件 **独立** 假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的 **联合概率分布**；然后基于此模型，对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$。

#####贝叶斯定理：
$$P(A|B)=\frac{P(B|A)\cdot P(A)}{P(B)}$$

+ **优点**： 在数据较少的情况下依然有效，可以处理多类别问题，接受大量数据训练，查询时具有高速度

+ **缺点**： 难以满足输入特征之间相互独立的前提

####2、支持向量机（SVM）
#####主要思想：
支持向量机是一种**二类分类模型**，他的基本模型是定义在特征空间上的**间隔最大的线性分类器，间隔最大使它有别于感知机**；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。


+ **优点**： SVM理论提供了一种避开高维空间，简化高维空间问题的求解难度的方法，具有较好的泛化推广能力。

+ **缺点**：  对于每个高维空间在此空间的映射如何确定，也就是核函数,现在还没有合适的方法；传统SVM进行二次规划的时候涉及到矩阵运算，因此对大规模的训练样本难以实施。

####3、K-最邻近
#####主要思想：
K-最近邻的思路是：如果一个样本在特征空间中的 k 个最相似即特征空间中最邻近的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN 算法中，所选择的邻居都是已经正确分类的对象。该方法在分类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。理论证明测试集很大，k也很大时，错误率趋向于理论最小值。


+ **优点**： S无需估计参数，无需训练；适合对稀有事件进行分类；特别适合于多分类问题

+ **缺点**： 计算量较大；当样本不平衡时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。

####4、逻辑斯谛回归
#####主要思想：
在统计学中，线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归,大于一个自变量情况的叫做多元回归。

回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。

在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做线性模型。

+ **优点**： 计算代价不高，易于理解和实现，且若采用随机梯度上升法可以在线学习；

+ **缺点**： 可能容易欠拟合，分类精度不高，难以找到足够的特征。