####35、常见机器学习算法比较
| 算法 |算法特点| 算法表示 | 损失函数 |关键表示|
| :---: | :----:|:----: | :----: | :----:|
| 朴素贝叶斯 | 基于特征条件独立假设，将最大后验概率作为输出，使用极大似然估计|$f(x)=\arg max_{c_k} P(Y=c_k)\\\cdot \sum_{j=1}^{n}P(x^j\arrowvert Y=c_k)$ | 0-1 损失 |生成模型，训练时学习联合概率分布，通过先验概率和条件概率求解，最后通过极大似然估计求解模型参数|
| 逻辑回归 | 是分类算法，对数线性模型，极大似然估计求模型参数|$p(y=1\arrowvert x)= \frac{e^{wx}}{1+e^{wx}}\\p(y=0\arrowvert x)= \frac{1}{1+e^{wx}}$ | $\prod_{i=1}^{n}\left[\pi(x_i)\right]^{y_i}\cdot\\\left[\pi(1-x_i)\right]^{1-y_i}$ |先将输入的数据映射到 $wx$ 上，其值域为实数域，这和回归模型非常类似|
| 最大熵模型 | 带约束概率模型，熵最大的模型是最好的模型，目标函数就是条件熵最大|$P_w(y\arrowvert x)=\frac{1}{Z_w(x)}e^\left(\sum_{i=1}^{n}w_if_i(x,y)\right)$ | 由特征函数$f(x,y)$决定 |最大熵模型和逻辑回归非常类似，目标是选择条件熵最大的模型，使用极大似然估计计算，带约束|
| 决策树 | 特征选择：信息增益（优先选择特征取值多的特征），信息增益比。生成：ID3和C4.5（对应两种特征选择）|$g(D,A)=H(D)-H(D\arrowvert A)\\g_R(D,A)=\frac{g(D,A)}{H(D\arrowvert A)}$ | 通常用于分类，正则化最大似然函数 |决策树是条件概率模型，树的深度影响模型复杂度，通常需要对决策树剪枝，提升泛化能力|
| CART | 特征选择：信息增益（优先选择特征取值多的特征），信息增益比。生成：ID3和C4.5（对应两种特征选择）|$g(D,A)=H(D)-H(D\arrowvert A)\\g_R(D,A)=\frac{g(D,A)}{H(D\arrowvert A)}$ | 通常用于分类，正则化最大似然函数 |决策树是条件概率模型，树的深度影响模型复杂度，通常需要对决策树剪枝，提升泛化能力|
| AdaBoost | 常用于分类，复杂分解成简单，是前向分步算法特例，贪心法带权重|$G(x)=sign\left(\sum_{m=1}^M\alpha_mG_m(x)\right)$ | $e_m = \frac{w_{mi}}{Z_m}\cdot e^{\alpha_m}$ |$\alpha_m = \frac{1}{2}log\frac{1-e_m}{e_m}\\Z_m=2\sqrt{e_m\cdot(1-e_m)}$|
| 回归提升树 | 常用于回归，复杂分解成简单，是前向分步算法特例，贪心法无权重|$f_M(x)=\sum_{m=1}^M T(x;\Theta_m)$ | $L = \left(y-f(x)\right)^2$ |$拟合残差,加入新树\\r_{mi} = y_i-f_{m-1}(x_i)$|
| 分类提升树 | 常用于分类，复杂分解成简单，是前向分步算法特例，贪心法无权重|$f_M(x)=\sum_{m=1}^M T(x;\Theta_m)$ | $L =exp \left(-y_if_i(x)\right)$ |将adaboost中二分类器变成二分类树，其他的和回归树相同，需要计算指数损失|
| GBDT | 只有回归树，可分类可回归，是前向分步算法特例，贪心法无权重|$f_M(x)=\sum_{m=1}^M T(x;\Theta_m)$ | $L = \left(y-f(x)\right)^2$ |$负梯度作为残差,关键区别\\r_{mi}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]$|
| XGBoost | 基本GBDT类似，但可用多种基本分类器，利用二阶导数，加入正则项|$L^t=\sum_{i=1}^{n}[g_if_t(x_i)+\\ \frac{1}{2}h_if_t^2(x_i)]+\Omega(f_t)$ | $\Omega=\gamma T +\frac{1}{2}\lambda\lVert w\rVert^2$ |$T$ 表示叶节点数量，$w$ 表示节点分数，选择信息增益最大的节点分裂，贪心思想|
| bagging | 随机有放回采样，基本分类器独立，boosting中基本分类器有依赖|$G(x)=\frac{1}{M}\sum_{m=i}^M G_m(x)$ | 同基本分类器，一般有决策树和神经网络 |回归问题：多个基本分类器平均，分类问题：投票选择|
| 随机森林 | 基于bagging思想，采用CART基分类器，随机选择部分特征来计算划分|$G(x)=\frac{1}{M}\sum_{m=i}^M G_m(x)$ | 采用CART基分类器计算损失 |回归问题：多个基本分类器平均，分类问题：投票选择|


####36、皮尔逊相关系数
皮尔逊相关系数也就是我们常说的相关系数。在概率统计中相关系数的定义如下：
$$\rho_{XY} = \frac{cov(X,Y)}{\sqrt{D(X)}\cdot \sqrt{D(Y)}}=\frac{E[(X-E(X))\cdot(Y-E(Y))]}{\sqrt{D(X)}\cdot \sqrt{D(Y)}}$$
其中，$E$ 表示数学期望，$D$ 表示方差，开根号之后就表示标准差。相关系数是衡量随机变量 $X$ 和 $Y$ 相关程度的一种方法，相关系数的取值范围是 $[-1,1]$ 和余弦相似度非常像。相关系数的绝对值越大，说明相关度越高。当 $X$ 和 $Y$ 线性相关时，那么相关系数为 $1(正相关)$ 或者 $-1(负相关)$。可以从以下几个角度去理解：
+ 当相关系数为 $0$ 时，$X$ 和 $Y$ 两个变量没有关系

+ 当 $X$ 的值增大（减少），$Y$ 的值增大（减少），那么二者正相关，相关系数在 $[0,1]$ 之间。

+ 当 $X$ 的值增大（减少），$Y$ 的值减少（增大），那么二者负相关，相关系数在 $[-1,0]$ 之间。

相关距离的定义为：
$$D_{XY}=1-\rho_{XY}$$

下面是相关系数与余弦相似度的比较：
先说结论: **皮尔逊相关系数是余弦相似度在维度值缺失情况下的一种改进**。
余弦相似度的公式为：
$$cos(a,b) = \frac{a\cdot b}{|a|\cdot |b|}$$

假设 $a = (3, 1, 0), b =  (2, -1, 2)$分子是 $a, b$ 两个向量的内积, $(3, 1, 0) \cdot (2, -1, 2) = 3*2 + 1*(-1) + 0*2 = 5 $
分母是两个向量模(模指的是向量的长度)的乘积.

然而，皮尔逊相关系数的公式可以如下表示：
$$sim(x,y) = \frac{\sum_{s \in s_{xy}}(r_{xs}-\bar{r_x})(r_{ys}-\bar{r_y})}{\sqrt{\sum_{s \in s_{xy}}(r_{xs}-\bar{r_x})^2}\sqrt{\sum_{s \in s_{xy}}(r_{ys}-\bar{r_y})^2}}$$

其实皮尔逊系数就是 $cos$ 计算之前两个向量都 **先进行中心化(centered)**，中心化的意思是说, 对每个向量, 我先计算所有元素的平均值, 然后向量中每个维度的值都减去这个, 得到的这个向量叫做被中心化的向量. 机器学习, 数据挖掘要计算向量余弦相似度的时候, 由于向量经常在某个维度上有数据的缺失, 预处理阶段都要对所有维度的数值进行中心化处理.

我们观察皮尔逊系数的公式:
+ 分子部分: 每个向量的每个数字要先减掉向量各个数字的平均值, 这就是在中心化.

+ 分母部分: 两个根号式子就是在做取模运算, 里面的所有的 $r$ 也要减掉平均值, 其实也就是在做中心化.


####37、KNN中K值的选择
在KNN算法中，有两个关键的指标，第一个就是距离的衡量，第二个就是邻居数量的选择。
+ 如果选择较小的 $K$ 值，就相当于用较小的邻域中的实例进行预测，通常而言，这种训练误差会减少，但是这种模型复杂度会相应的增加，容易发生过拟合。

+ 如果选择较大的 $K$ 值，那么这样的模型会更简单，但是训练误差会增加。

举个极端的例子，如果 $K=N$ 那么预测的类型就是最多的哪一类，这种模型非常简单，但是明显可以发现效果非常差。在实际中，通常先选择一个较小的 $K$ 值，然后再进行交叉验证，来选择最优的 $K$ 值。



####38、请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么？
集成学习的集成对象是学习器. Bagging和Boosting属于集成学习的两类方法. Bagging方法有放回地采样同数量样本训练每个学习器, 然后再一起集成(简单投票); Boosting方法使用全部样本(可调权重)依次训练每个学习器, 迭代集成(平滑加权).

决策树属于最常用的学习器, 其学习过程是从根建立树, 也就是如何决策叶子节点分裂. ID3/C4.5决策树用信息熵计算最优分裂, CART决策树用基尼指数计算最优分裂, xgboost决策树使用二阶泰勒展开系数计算最优分裂.

下面所提到的学习器都是决策树:
Bagging方法:
+ 学习器间不存在强依赖关系, 学习器可并行训练生成, 集成方式一般为投票;

+ Random Forest属于Bagging的代表, 放回抽样, 每个学习器随机选择部分特征去优化;

Boosting方法:
+ 学习器之间存在强依赖关系、必须串行生成, 集成方式为加权和;

+ Adaboost属于Boosting, 采用指数损失函数替代原本分类任务的0/1损失函数;

+ GBDT属于Boosting的优秀代表, 对函数残差近似值进行梯度下降, 用CART回归树做学习器, 集成为回归模型;

+ xgboost属于Boosting的集大成者, 对函数残差近似值进行梯度下降, 迭代时利用了二阶梯度信息, 集成模型可分类也可回归. 由于它可在特征粒度上并行计算, 结构风险和工程实现都做了很多优化, 泛化, 性能和扩展性都比GBDT要好。

xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有：
 + 1.损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数

 + 2.对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性

 + 3.节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的

































