##<center>统计学习方法总结</center>
阅读李航老师的统计学习方法，主要学习了感知机、$k$ 近邻法、朴素贝叶斯法、决策树、逻辑回归和最大熵模型、支持向量机、提升方法、EM算法、隐马尔科夫模型和条件随机场共10种主要的统计学习方法，这些统计学习方法的特定概率总结如下表：

| 方法 | 适用问题 | 模型特点 | 类型| 学习策略 |损失函数 | 学习算法|
| :--- | :----: | :----: | :----: | :----: | :----: | :----: |
|感知机 | 二分类 | 分离超平面 |判别 |极小化误分类点到超平面距离 |误分类点到超平面距离 |随机梯度下降 |
|$k$ 近邻法 | 多类分类，回归 | 特征空间，样本点 |判别 | | | |
|朴素贝叶斯 | 多类分类 |特征与类别的联合概率分布，条件独立假设 | <font color="red">生成</font> |极大似然估计 |对数似然损失 |概率计算公式，EM算法 |
|决策树 |多类分类，回归 | 分类树，会归树 |判别 |正则化的极大似然估计 |对数似然损失 |特征选择，生产，剪枝 |
|逻辑回归与最大熵模型 |多类分类|特征条件下类别的条件概率分布，对数线性模型 |判别 |极大似然估计，正则化的极大似然估计 |逻辑斯蒂损失 |改进的迭代尺度算法，梯度下降，拟牛顿法|
|支持向量机 | 二分类 | 分离超平面，核技巧 |判别 |极小化正则化合页损失，软间隔最大化 |合页损失 |序列最小最优化算法（SMO） |
|提升方法 | 二分类 | 弱分类器的线性组合|判别 |极小化加法模型的指数损失 |指数损失 |前向分步加法算法 |
|EM算法 | 概率模型参数估计 | 含隐变量的概率模型| |极大似然估计，极大后验概率估计 |对数似然损失 |迭代算法 |
|隐马尔科夫模型 | 标注 | 观测序列与状态序列的联合概率分布模型|<font color="red">生成</font> |极大似然估计，极大后验概率估计 |对数似然损失 |概率计算公式，EM算法 |
|条件随机场 | 标注 | 状态序列条件下观测寻列的条件概率分布，对数线性模型|判别 |极大似然估计，极大后验概率估计 |对数似然损失 |改进的迭代尺度算法，梯度下降，拟牛顿法|

下面对各种方法的特点及其关系进行简单的讨论：
#####1、适用问题
本书主要介绍的是监督学习方法，监督学习方法可以认为是学习一个模型，是它能对给定的输入预测相应的输出。监督学习包括分类，回归，标注等。分类问题是从实例的特征向量到类标记的预测问题，标注问题是从观测序列到标记序列（或状态序列）的预测问题，例如，看到一段文字（状态序列），对这段文字分词（标记序列），当前的标记会影响后面的标记预测。可以认为分类问题是观测问题的特殊情况。分类问题中可能的预测结果为二分类或者多分类。而标注问题可能的预测结果是所有可能标记组合，是指数数量级的。

感知机、$k$ 近邻法、朴素贝叶斯、决策树、逻辑回归与最大熵模型、支持向量机、提升方法是分类问题。原始的感知机，支持向量机以及提升方法是针对二分类的，可以将其扩展到多分类中。隐马尔科夫模型，条件随机场是标注方法，EM算法是含有隐变量的概率模型的一般学习算法，可以用于生产模型的非监督学习。

感知机、$k$ 近邻法、朴素贝叶斯、决策树是简单的分类方法，具有模型直观，方法简单，实现容易等特点。逻辑斯蒂与最大熵模型，支持向量机，提升方法是更复杂的但是更有效的分类方法。往往分类精度比较高，隐马尔科夫模型、条件随机场是主要的标注方法，通常来说，条件随机场标注准确率更高。

#####2、模型
分类问题与标注问题的预测模型都可以认为适合表示输入空间到输出空间的映射。它们可以写成条件概率分布$P(Y|X)$ 或者决策函数 $Y=f(X)$ 的形式。前者表示给定输入条件下输出的概率模型，后者表示输入到输出的非概率模型。

直接学习条件概率分布 $P(Y|X)$ 或决策函数 $Y=f(X)$ 的方法为判别方法，对应的模型是判别模型。首先学习联合概率分布 $P(X,Y)$，从而求得条件概率分布 $P(Y|X)$ 的方法是生成方法，对应的模型是生成模型。通常，可以使用非监督学习的方法学习生成模型，**应用EM算法来实现**。

#####3、学习策略
在二分类的监督学习中，支持向量机，逻辑斯蒂回归与最大熵模型，提升方法分布使用合页损失函数，逻辑斯蒂损失函数，指数损失函数。这三种函数表示如下：
$$合页损失函数：[1-yf(x)]_+$$
$$逻辑斯蒂损失函数：log[1+e^{-yf(x)}]$$
$$指数损失：e^{-yf(x)}$$

这三种损失函数都是0-1损失函数的上界，具有相似的形状，如下图所示，所以**针对不同的分类问题，可以定义不同的损失函数**。
<center><img src="./img/1.png"/></center>.

通常一个分类模型的损失函数包括经验风险和结构风险。支持向量机使用 $L_2$ 范数表示模型的复杂度，原始的逻辑斯蒂回归于最大熵模型没有正则化项，可以增加 $L_2$ 正则项。提升方法没有明显的正则化项，通常使用早停止的方法达到正则化得效果。
